services:
  # One runtime per mock instance (Option A): each runtime gets its own Docker IP on the control-plane network.
  # The worker-agent shares the same network namespace/IP as mock-vllm (like a real VM: one IP, multiple ports).
  mock-vllm:
    # Use pre-built image to avoid build context issues when running from orchestrator container
    # Image should be built with: docker build -t inventiv-mock-vllm:latest -f inventiv-worker-mock/Dockerfile.mock-vllm inventiv-worker-mock/
    image: inventiv-mock-vllm:latest
    environment:
      - MODEL_ID=${MOCK_VLLM_MODEL_ID:-demo-model}
      - PORT=8000
      - VLLM_NUM_REQUESTS_WAITING=${MOCK_VLLM_REQUESTS_WAITING:-7}
      - VLLM_NUM_REQUESTS_RUNNING=${MOCK_VLLM_REQUESTS_RUNNING:-1}
    networks:
      - controlplane

  worker-agent:
    build:
      context: ./inventiv-worker
      dockerfile: Dockerfile.agent
    # Same IP as mock-vllm: vLLM on :8000, health/metrics on :8080.
    network_mode: "service:mock-vllm"
    environment:
      - CONTROL_PLANE_URL=http://api:8003
      - WORKER_AUTH_TOKEN=${WORKER_AUTH_TOKEN:-dev-worker-token}
      - INSTANCE_ID=${INSTANCE_ID:?INSTANCE_ID is required}
      - MODEL_ID=${MOCK_VLLM_MODEL_ID:-demo-model}
      # Enable synthetic GPU metrics in mock runtimes (no nvidia-smi in local/docker).
      - WORKER_SIMULATE_GPU_COUNT=${WORKER_SIMULATE_GPU_COUNT:-1}
      - WORKER_SIMULATE_GPU_VRAM_MB=${WORKER_SIMULATE_GPU_VRAM_MB:-24576}
      - VLLM_BASE_URL=http://127.0.0.1:8000
      - WORKER_HEALTH_PORT=8080
      - WORKER_VLLM_PORT=8000
      - WORKER_HEARTBEAT_INTERVAL_S=5
      - PYTHONUNBUFFERED=1
    depends_on:
      - mock-vllm

networks:
  controlplane:
    external: true
    name: ${CONTROLPLANE_NETWORK_NAME:?CONTROLPLANE_NETWORK_NAME is required (e.g. inventiv-agents-worker-fixes_default)}


