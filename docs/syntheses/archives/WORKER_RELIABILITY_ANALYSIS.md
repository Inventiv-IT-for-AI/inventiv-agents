# Analyse de Fiabilit√© des Workers et Gestion des Instances

**Date**: 2026-01-06  
**Objectif**: Identifier les points d'am√©lioration pour la fiabilisation de la gestion des Instances et des workers

---

## üìã R√©sum√© Ex√©cutif

Cette analyse identifie les points critiques et propose des am√©liorations pour :
1. **Fiabilit√© des workers** : gestion des heartbeats, d√©tection de d√©faillances, r√©cup√©ration automatique
2. **Gestion des instances** : transitions d'√©tat robustes, d√©tection d'instances bloqu√©es, nettoyage automatique
3. **Observabilit√©** : logging structur√©, m√©triques, alertes

---

## üèóÔ∏è Architecture Actuelle (R√©sum√©)

### Composants Principaux

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Frontend  ‚îÇ (Next.js :3000)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ HTTP (session JWT)
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  inventiv-  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    Redis     ‚îÇ (Pub/Sub: CMD:*, EVT:*)
‚îÇ    api      ‚îÇ      ‚îÇ  (Events)    ‚îÇ
‚îÇ   (:8003)   ‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
       ‚îÇ                    ‚îÇ Subscribe
       ‚îÇ PostgreSQL         ‚ñº
       ‚îÇ (State)      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  inventiv-   ‚îÇ
                       ‚îÇ orchestrator ‚îÇ (Control Plane :8001)
                       ‚îÇ  (Jobs/State)‚îÇ
                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚îÇ Provider API
                              ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ Scaleway / Mock  ‚îÇ
                    ‚îÇ  (Instances GPU) ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚îÇ Worker Agent
                              ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ inventiv-worker ‚îÇ
                    ‚îÇ (vLLM + Agent)   ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Jobs Background (Orchestrator)

1. **job-health-check** (`health_check_job.rs`)
   - Intervalle: 10s
   - Traite: `booting`, `installing`, `starting` ‚Üí `ready` / `startup_failed`
   - Utilise `SKIP LOCKED` pour √©viter les conflits

2. **job-provisioning** (`provisioning_job.rs`)
   - Intervalle: 10s
   - Re-queue les instances `provisioning` bloqu√©es (Redis Pub/Sub non durable)

3. **job-terminator** (`terminator_job.rs`)
   - Intervalle: 10s
   - Traite: `terminating` ‚Üí `terminated`
   - Supprime les volumes (`delete_on_terminate=true`)

4. **job-watch-dog** (`watch_dog_job.rs`)
   - Intervalle: 10s
   - D√©tecte les instances `ready` supprim√©es chez le provider (orphan detection)

5. **job-recovery** (`recovery_job.rs`)
   - Intervalle: 30s
   - R√©cup√®re les instances bloqu√©es dans divers √©tats

### Bus d'√âv√©nements Redis

**Channels**:
- `orchestrator_events`: `CMD:*` (PROVISION, TERMINATE, SYNC_CATALOG, RECONCILE)
- `finops_events`: `EVT:*` (co√ªts, tokens)

**Garanties**: Non-durable Pub/Sub ‚Üí requeue si orchestrator down

---

## üîç Points Critiques Identifi√©s

### 1. Gestion des Heartbeats Workers

#### √âtat Actuel
- ‚úÖ Heartbeats re√ßus via `/internal/worker/heartbeat` (proxy via API)
- ‚úÖ Mise √† jour `worker_last_heartbeat` dans DB
- ‚úÖ R√©cup√©ration automatique si `startup_failed` avec `STARTUP_TIMEOUT`
- ‚ö†Ô∏è Pas de d√©tection explicite de workers "morts" (heartbeats arr√™t√©s)

#### Probl√®mes Potentiels
1. **Workers silencieux** : Si un worker cesse d'envoyer des heartbeats mais reste `ready`, l'instance reste marqu√©e `ready` ind√©finiment
2. **Timeout heartbeat** : Pas de logique explicite pour marquer une instance comme "worker_dead" si `worker_last_heartbeat` > seuil
3. **R√©cup√©ration partielle** : Le heartbeat peut r√©cup√©rer `startup_failed` ‚Üí `booting`, mais pas d√©tecter les workers qui meurent apr√®s `ready`

#### Recommandations
- [ ] Ajouter un job `job-worker-watchdog` qui d√©tecte les workers sans heartbeat r√©cent (> 5 min) en √©tat `ready`
- [ ] Transition automatique `ready` ‚Üí `worker_dead` si heartbeat > seuil (configurable)
- [ ] Option de r√©installation automatique pour les workers morts

### 2. Health Checks et Readiness

#### √âtat Actuel
- ‚úÖ Health checks multiples : SSH (port 22), Worker `/readyz`, vLLM `/v1/models`
- ‚úÖ Priorit√© aux heartbeats r√©cents (< 30s) sur les checks actifs
- ‚úÖ Support des √©tats interm√©diaires : `booting` ‚Üí `installing` ‚Üí `starting` ‚Üí `ready`
- ‚ö†Ô∏è Timeouts fixes (2h pour `booting`, 30min pour model loading)

#### Probl√®mes Potentiels
1. **Timeouts trop longs** : 2h pour `booting` peut masquer des probl√®mes r√©els
2. **Health checks co√ªteux** : Appels SSH r√©p√©t√©s peuvent √™tre lents
3. **Pas de backoff exponentiel** : Health checks √† intervalle fixe (10s) m√™me pour instances probl√©matiques

#### Recommandations
- [ ] Impl√©menter un backoff exponentiel pour les health checks √©chou√©s
- [ ] R√©duire les timeouts par d√©faut (configurables via env vars)
- [ ] Ajouter des m√©triques de latence des health checks
- [ ] Cache des r√©sultats de health checks (√©viter appels r√©p√©t√©s < 30s)

### 3. Gestion des Instances Bloqu√©es

#### √âtat Actuel
- ‚úÖ `job-recovery` d√©tecte les instances `booting` bloqu√©es > 2h
- ‚úÖ `job-provisioning` re-queue les instances `provisioning` bloqu√©es
- ‚úÖ `job-terminator` g√®re les instances `terminating` bloqu√©es
- ‚ö†Ô∏è Pas de d√©tection pour `installing` / `starting` bloqu√©es

#### Probl√®mes Potentiels
1. **√âtats interm√©diaires** : `installing` et `starting` peuvent rester bloqu√©s sans r√©cup√©ration
2. **Retry limits** : `retry_count < 5` dans `provisioning_job`, mais pas de limite globale
3. **Pas de notification** : Instances bloqu√©es > seuil ne g√©n√®rent pas d'alertes

#### Recommandations
- [ ] √âtendre `job-recovery` pour d√©tecter `installing` / `starting` bloqu√©es
- [ ] Ajouter un syst√®me d'alertes (logs structur√©s + m√©triques) pour instances bloqu√©es
- [ ] Impl√©menter un circuit breaker pour instances avec trop d'√©checs

### 4. Gestion des Volumes

#### √âtat Actuel
- ‚úÖ D√©couverte automatique des volumes attach√©s (`list_attached_volumes`)
- ‚úÖ Tracking dans `instance_volumes` avec `delete_on_terminate`
- ‚úÖ Suppression automatique lors de la terminaison
- ‚ö†Ô∏è Pas de nettoyage p√©riodique des volumes orphelins

#### Probl√®mes Potentiels
1. **Volumes orphelins** : Si une instance est supprim√©e manuellement chez le provider, les volumes peuvent rester
2. **√âchecs de suppression** : Pas de retry automatique si suppression √©choue
3. **Pas de r√©conciliation** : Pas de job d√©di√© pour r√©concilier les volumes DB vs provider

#### Recommandations
- [ ] Ajouter un job `job-volume-reconciliation` pour d√©tecter les volumes orphelins
- [ ] Retry automatique avec backoff pour les suppressions √©chou√©es
- [ ] Alertes pour volumes non supprim√©s apr√®s terminaison

### 5. Observabilit√© et Logging

#### √âtat Actuel
- ‚úÖ Logging structur√© dans `action_logs`
- ‚úÖ Worker event logging (`/logs` endpoint)
- ‚úÖ M√©triques worker (GPU, queue depth, etc.)
- ‚ö†Ô∏è Pas de m√©triques Prometheus pour les jobs
- ‚ö†Ô∏è Pas de traces distribu√©es (correlation_id partiellement impl√©ment√©)

#### Probl√®mes Potentiels
1. **M√©triques manquantes** : Pas de m√©triques pour latence des jobs, taux d'√©chec, etc.
2. **Corr√©lation limit√©e** : `correlation_id` pr√©sent mais pas utilis√© partout
3. **Pas d'alertes** : Pas de syst√®me d'alertes pour incidents critiques

#### Recommandations
- [ ] Exposer des m√©triques Prometheus pour tous les jobs
- [ ] Impl√©menter un syst√®me d'alertes bas√© sur les m√©triques
- [ ] √âtendre l'utilisation de `correlation_id` pour le tracing end-to-end

---

## üéØ Plan d'Action Prioris√©

### Phase 1 : Am√©liorations Critiques (1-2 semaines)

#### 1.1 D√©tection des Workers Morts
- [ ] Cr√©er `job-worker-watchdog.rs` pour d√©tecter workers sans heartbeat r√©cent
- [ ] Ajouter transition `ready` ‚Üí `worker_dead` si heartbeat > 5 min
- [ ] Tests unitaires et E2E

#### 1.2 Am√©lioration des Health Checks
- [ ] Impl√©menter backoff exponentiel pour health checks √©chou√©s
- [ ] R√©duire timeouts par d√©faut (configurables)
- [ ] Ajouter cache des r√©sultats (< 30s)

#### 1.3 Extension du Job Recovery
- [ ] D√©tecter `installing` / `starting` bloqu√©es > seuil
- [ ] Ajouter alertes (logs structur√©s) pour instances bloqu√©es

### Phase 2 : Am√©liorations Importantes (2-4 semaines)

#### 2.1 R√©conciliation des Volumes
- [ ] Cr√©er `job-volume-reconciliation.rs`
- [ ] D√©tecter volumes orphelins (DB vs provider)
- [ ] Retry automatique avec backoff pour suppressions √©chou√©es

#### 2.2 M√©triques et Observabilit√©
- [ ] Exposer m√©triques Prometheus pour tous les jobs
- [ ] Ajouter m√©triques de latence et taux d'√©chec
- [ ] Dashboard Grafana (optionnel)

#### 2.3 Circuit Breaker
- [ ] Impl√©menter circuit breaker pour instances avec trop d'√©checs
- [ ] Configurer seuils (ex: 5 √©checs cons√©cutifs ‚Üí circuit ouvert)

### Phase 3 : Am√©liorations Optionnelles (1-2 mois)

#### 3.1 Syst√®me d'Alertes
- [ ] Int√©gration avec syst√®me d'alertes (ex: Alertmanager)
- [ ] Alertes pour incidents critiques (instances bloqu√©es, workers morts)

#### 3.2 Tracing Distribu√©
- [ ] √âtendre utilisation de `correlation_id` partout
- [ ] Int√©gration OpenTelemetry (optionnel)

---

## üìä M√©triques Cl√©s √† Surveiller

### Workers
- Taux de heartbeats re√ßus / attendus
- Latence des heartbeats (p50, p95, p99)
- Nombre de workers morts d√©tect√©s
- Temps moyen de r√©cup√©ration apr√®s d√©faillance

### Instances
- Temps moyen de transition `provisioning` ‚Üí `ready`
- Taux d'√©chec par √©tat (`provisioning_failed`, `startup_failed`)
- Nombre d'instances bloqu√©es par √©tat
- Temps moyen de terminaison

### Jobs
- Latence d'ex√©cution par job (p50, p95, p99)
- Taux d'erreur par job
- Nombre d'instances trait√©es par cycle

### Volumes
- Nombre de volumes orphelins
- Taux de succ√®s de suppression
- Temps moyen de suppression

---

## üîß Configuration Recommand√©e

### Variables d'Environnement √† Ajouter

```bash
# Worker Watchdog
WORKER_HEARTBEAT_TIMEOUT_SECONDS=300  # 5 minutes
WORKER_DEAD_RECOVERY_ENABLED=true

# Health Checks
HEALTH_CHECK_BACKOFF_ENABLED=true
HEALTH_CHECK_BACKOFF_MAX_INTERVAL_SECONDS=300
HEALTH_CHECK_CACHE_TTL_SECONDS=30

# Recovery
RECOVERY_INSTALLING_TIMEOUT_SECONDS=3600  # 1 hour
RECOVERY_STARTING_TIMEOUT_SECONDS=1800     # 30 minutes
RECOVERY_BOOTING_TIMEOUT_SECONDS=7200      # 2 hours

# Volume Reconciliation
VOLUME_RECONCILIATION_ENABLED=true
VOLUME_DELETE_RETRY_MAX_ATTEMPTS=5
VOLUME_DELETE_RETRY_BACKOFF_SECONDS=60

# Circuit Breaker
CIRCUIT_BREAKER_ENABLED=true
CIRCUIT_BREAKER_FAILURE_THRESHOLD=5
CIRCUIT_BREAKER_RESET_TIMEOUT_SECONDS=300
```

---

## üìù Notes de Conception

### Principe de R√©cup√©ration
- **Idempotence** : Tous les jobs doivent √™tre idempotents (r√©ex√©cution s√ªre)
- **SKIP LOCKED** : Utiliser `FOR UPDATE SKIP LOCKED` pour √©viter les conflits entre orchestrators multiples
- **Backoff exponentiel** : √âviter les appels r√©p√©t√©s co√ªteux
- **Graceful degradation** : Continuer √† fonctionner m√™me si certains composants √©chouent

### Gestion des Erreurs
- **Logging structur√©** : Tous les √©v√©nements critiques doivent √™tre logg√©s dans `action_logs`
- **M√©tadonn√©es** : Inclure `correlation_id`, `retry_count`, `error_code` dans les logs
- **Retry intelligent** : Distinguer erreurs temporaires (retry) vs permanentes (fail fast)

---

## üß™ Tests Recommand√©s

### Tests Unitaires
- [ ] Tests pour `job-worker-watchdog` (d√©tection workers morts)
- [ ] Tests pour backoff exponentiel
- [ ] Tests pour circuit breaker

### Tests d'Int√©gration
- [ ] Test E2E : Worker meurt apr√®s `ready` ‚Üí d√©tection ‚Üí r√©cup√©ration
- [ ] Test E2E : Instance bloqu√©e dans `installing` ‚Üí r√©cup√©ration
- [ ] Test E2E : Volume orphelin ‚Üí r√©conciliation ‚Üí suppression

### Tests de Charge
- [ ] Test avec 100+ instances simultan√©es
- [ ] Test avec orchestrator red√©marrage (requeue)
- [ ] Test avec provider API lent/intermittent

---

## üìö R√©f√©rences

- [Architecture](architecture.md)
- [State Machine & Progress](STATE_MACHINE_AND_PROGRESS.md)
- [Worker & Router Phase 0.2](worker_and_router_phase_0_2.md)
- [Agent Version Management](AGENT_VERSION_MANAGEMENT.md)
- [Storage Management](STORAGE_MANAGEMENT.md)

---

## ‚úÖ Checklist de Validation

Avant de consid√©rer les am√©liorations comme compl√®tes :

- [ ] Tous les jobs ont des m√©triques Prometheus
- [ ] Tous les timeouts sont configurables via env vars
- [ ] Tous les jobs utilisent `SKIP LOCKED` pour √©viter conflits
- [ ] Tous les √©v√©nements critiques sont logg√©s dans `action_logs`
- [ ] Tests unitaires et d'int√©gration passent
- [ ] Documentation mise √† jour
- [ ] Migration DB si n√©cessaire

